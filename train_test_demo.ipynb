{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    def __init__(self, data, _prev = (), _op = ''):\n",
    "        self.data = data\n",
    "        self.grad = 0.0\n",
    "        self._backward = None \n",
    "        self._prev = _prev\n",
    "        self._op = _op\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data})\"\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), _op='+')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data - other.data, (self, other), _op='-')\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += -1.0 * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        return self - other\n",
    "    \n",
    "    def  __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), _op='*')\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __pow__(self, x):\n",
    "        assert isinstance(x, (int, float)), \"Only supporting scalar exponentiation\"\n",
    "        out = Value(self.data ** x, (self, ), _op=f'**{x}')\n",
    "        def _backward():\n",
    "            self.grad += (x * self.data**(x-1)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other**-1\n",
    "    \n",
    "    # def __rdiv__(self, other):\n",
    "    #     return self * other**-1\n",
    "\n",
    "    def exp(self): # e**x\n",
    "        out = Value(math.exp(self.data), (self, ), _op='exp')\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def sigmoid(self): # Sigmoid\n",
    "        x = self.data\n",
    "        sig = 1 / (1 + math.exp(-x))\n",
    "        #sig = (1 + (-self).exp())**-1\n",
    "        out = Value(sig, (self, ), _op='sigmoid')\n",
    "        def _backward():\n",
    "            self.grad += (sig * (1 - sig)) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def silu(self): #Sigmoid Linear Unit (Swish)\n",
    "        x = self.data\n",
    "        sig = self.sigmoid().data\n",
    "        out = Value(x * sig, (self, ), _op='silu')\n",
    "        def _backward():\n",
    "            self.grad += ((x * (sig * (1 - sig))) + sig) * out.grad\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def backprop(self): #use topological sort for all children and reverse the result\n",
    "        values = []\n",
    "        seen = set()\n",
    "        def build_topo(self):\n",
    "            stack = [self]\n",
    "            while stack:\n",
    "                node = stack.pop()\n",
    "                if node not in seen:\n",
    "                    seen.add(node)\n",
    "                    stack.extend(node._prev)  # Add parents to stack\n",
    "                    values.append(node)  # Add self AFTER parents\n",
    "        build_topo(self)\n",
    "\n",
    "        for v in values:\n",
    "            v.grad = 0.0 # Reset gradients from previous backpropagation step\n",
    "        self.grad = 1.0 # Seed gradient for this node (starting point for backprop)\n",
    "\n",
    "        [v._backward() for v in values if v._backward is not None] #backprop from the last node (BFS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, count: int):\n",
    "        # act(w * x + b)\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(count)]\n",
    "        self.b = Value(random.uniform(-1, 1))\n",
    "\n",
    "    def act(self, xs: list): \n",
    "        return (sum([wi * xi for wi, xi in zip(self.w, xs)]) + self.b).silu()\n",
    "\n",
    "    def __repr__(self): # neat printout\n",
    "        return '\\n'.join([f\"    Weight {i+1}: {self.w[i].data:12.7f}\" for i in range(len(self.w))]) + f\"\\n   \\tBias: {self.b.data:14.7f}\"\n",
    "class Layer:\n",
    "    def __init__(self, nin, nout):\n",
    "        self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f\"  Neuron {i+1}\\n{self.neurons[i]}\" for i in range(len(self.neurons))])\n",
    "\n",
    "    def act(self, xs: list):\n",
    "        return [n.act(xs) for n in self.neurons]\n",
    "    \n",
    "class MLP:\n",
    "    def __init__(self, layers: list): # [1, 3, 5, 3, 1] -> 1,3;3,5\n",
    "        self.layers = [Layer(layers[i], layers[i+1]) for i in range(len(layers)-1)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '\\n'.join([f\"Layer {i+1}\\n{self.layers[i]}\" for i in range(len(self.layers))])\n",
    "    \n",
    "    def all_values(self):\n",
    "        values = []\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                for wt in neuron.w:\n",
    "                    values.append(wt)\n",
    "                values.append(neuron.b)\n",
    "        return values\n",
    "    \n",
    "    def __call__(self, xs: list):\n",
    "        for layer in self.layers:\n",
    "            pred = layer.act(xs)\n",
    "        return pred\n",
    "    \n",
    "    def train(self, input, desired_output, epochs: int, step_size: int):\n",
    "        for i in range(epochs):\n",
    "            preds = [self(x) for x in input]\n",
    "            n_preds = []\n",
    "            for sublist in preds:\n",
    "                for pred in sublist:\n",
    "                    n_preds.append(pred)\n",
    "            loss = sum([(ground_truth - pred)**2 for ground_truth, pred in zip(desired_output, n_preds)])\n",
    "            loss.backprop()\n",
    "            for val in self.all_values():\n",
    "                val.data += -step_size * val.grad\n",
    "        print(f\"Trained for {i+1} epochs. loss: {loss.data:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MLP([1,3,5,7,5,3,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for 1000 epochs. loss: 1.09981\n"
     ]
    }
   ],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired\n",
    "\n",
    "a.train(xs, ys, 1000, 0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
